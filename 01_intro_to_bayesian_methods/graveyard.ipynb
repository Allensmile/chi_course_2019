{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Bayesian updating\n",
    "\n",
    "We can show a simple example. Imagine trying to estimate the altitude of a plane attempting to land near an airstrip. We receive radar altitude measurements; these are inaccurate and occasionally. We have a rough idea of how high the plane might be (it's not likely to be trying to land from 10 km up), and very rough idea of how fast it is descending.\n",
    "\n",
    "If we can define ways of sampling from these distributions, we can then update our beliefs. We will do this by \"copying\" samples which correspond to more likely states. This turns out to be a well-founded algorithm for Bayesian estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def redraw_figure(fig):\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    IPython.display.display(fig)\n",
    "    time.sleep(0.25)\n",
    "    \n",
    "    \n",
    "def estimate_landing(iters=100, noise=0.1, k=5):\n",
    "    n_samples = 50\n",
    "    alt_guess = np.random.normal(500, 500, n_samples)\n",
    "    vsi_guess = np.random.normal(-50, 10, n_samples)\n",
    "    \n",
    "    \n",
    "    alt_true = 500\n",
    "    vsi_true = -47.5\n",
    "    \n",
    "    # set up a figure\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.set_ylim(-0.5, 700)\n",
    "    ax.set_ylabel(\"Altitude (m)\")\n",
    "    ax.set_xlabel(\"Time (seconds)\")\n",
    "    ax.set_xlim(-1, iters+1)\n",
    "    \n",
    "    for ix in range(iters):\n",
    "    \n",
    "        \n",
    "        # make a noisy observation\n",
    "        alt_obs = alt_true + np.random.normal(0, noise)\n",
    "        \n",
    "        # update our \"true\" model\n",
    "        alt_true += vsi_true\n",
    "        vsi_true *= 0.95\n",
    "        \n",
    "        ## LIKELIHOOD\n",
    "        # compute a weighting function \n",
    "        u_weights = np.exp(-(alt_guess - alt_obs)**2 * k)\n",
    "        \n",
    "        \n",
    "        weights = u_weights / np.sum(u_weights) # normalise\n",
    "        \n",
    "        \n",
    "        ax.text(ix, alt_true, 'âœˆ', size=50, horizontalalignment='center', color='C3', alpha=0.2)\n",
    "        ax.set_title(\"True\")\n",
    "        redraw_figure(fig)\n",
    "        #ax.plot([ix, ix], [alt_obs-2*noise, alt_obs+2*noise], 'C2')\n",
    "        #ax.plot([ix, ix], [alt_obs-noise, alt_obs+noise], 'C2', lw=4)\n",
    "        ax.plot(ix, alt_obs, 'C4o', markersize=20, alpha=0.8)\n",
    "        ax.set_title(\"Observed\")\n",
    "        redraw_figure(fig)\n",
    "        ax.scatter(x=np.tile(ix, (len(alt_guess))), y=alt_guess, s=u_weights*100, \n",
    "                   color='C0', zorder=10, edgecolor='black', alpha=0.5)\n",
    "        ax.set_title(\"Estimates\")\n",
    "        redraw_figure(fig)\n",
    "        \n",
    "        ax.plot(ix, np.sum(alt_guess * weights), 'C1o', markersize=20, zorder=100)\n",
    "        redraw_figure(fig)\n",
    "        \n",
    "        \n",
    "        ## SAMPLING        \n",
    "        # copy the particles, favouring heavily weighted ones\n",
    "        copies = pfilter.resample(weights)\n",
    "        \n",
    "        alt_guess = alt_guess[copies]\n",
    "        vsi_guess = vsi_guess[copies]\n",
    "        \n",
    "        # APPLY FUNCTION TO SAMPLES\n",
    "        # update our position estimate\n",
    "        # internal model\n",
    "        alt_guess += vsi_guess\n",
    "        vsi_guess *= 0.95\n",
    "        \n",
    "        # DIFFUSE              \n",
    "        # add a little noise\n",
    "        alt_guess += np.random.normal(0, 20, alt_guess.shape)\n",
    "        vsi_guess += np.random.normal(0, 2, vsi_guess.shape)\n",
    "        #ax.scatter(x=np.tile(ix+0.5, (len(alt_guess))), y=alt_guess, \n",
    "        #           s=10, color='C2', zorder=10)\n",
    "        #ax.set_title(\"Predict\")\n",
    "        #redraw_figure(fig)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ax.set_ylim(-0.5, 700)\n",
    "        ax.set_ylabel(\"Altitude (m)\")\n",
    "        ax.set_xlabel(\"Time (seconds)\")\n",
    "        ax.set_xlim(-1, iters+1)\n",
    "                \n",
    "        \n",
    "estimate_landing(noise=100, k=8e-5, iters=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_coin(observations):\n",
    "    n_samples = 20\n",
    "    bias_samples = np.random.uniform(0, 1, n_samples)\n",
    "    k = 5\n",
    "    \n",
    "    # set up a figure\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.set_ylim(-0.5, 1.5)\n",
    "    ax.set_xlim(-1, len(observations)+1)\n",
    "    \n",
    "    for ix, observed in enumerate(observations):        \n",
    "        ax.plot(ix, observed, 'C1d')\n",
    "        \n",
    "        \n",
    "        # guess what we *would* observe\n",
    "        guessed_observations = np.random.binomial(n=1, p=np.tile(bias_samples, (50, 1)))\n",
    "        print(guessed_observations.shape)\n",
    "        \n",
    "        # compute a weighting function\n",
    "        weights = np.sum(np.exp(-(observed - guessed_observations)**2 * k), axis=0)\n",
    "        weights = weights / np.sum(weights) # normalise\n",
    "        \n",
    "        ax.scatter(x=np.tile(ix, (len(bias_samples))), y=bias_samples, s=weights*500, color='C0')\n",
    "        # copy the particles, favouring heavily weighted ones\n",
    "        copies = pfilter.resample(weights)\n",
    "        \n",
    "        for i in range(len(bias_samples)):\n",
    "            ax.plot([ix, ix+1], [bias_samples[i], bias_samples[copies[i]]], 'C0-', alpha=0.2)\n",
    "        \n",
    "        bias_samples = bias_samples[copies]\n",
    "                        \n",
    "        \n",
    "        # add a little noise\n",
    "        bias_samples = np.clip(bias_samples + np.random.normal(0,0.01,n_samples), 0, 1.0)\n",
    "                \n",
    "        \n",
    "estimate_coin(np.random.binomial(n=1, p=0.75, size=30))        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
